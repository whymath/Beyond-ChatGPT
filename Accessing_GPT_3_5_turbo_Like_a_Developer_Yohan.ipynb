{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-3.5-turbo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4SKfBCSB8ds",
        "outputId": "db790e6e-5133-4565-8f53-97b0a3fcfe6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (1.13.3)\n",
            "Requirement already satisfied: cohere in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (4.52)\n",
            "Requirement already satisfied: tiktoken in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from openai) (2.6.3)\n",
            "Requirement already satisfied: sniffio in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from cohere) (3.9.3)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from cohere) (1.9.4)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from cohere) (6.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: certifi in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: colorama in d:\\workspaces\\aimakerspace\\beyond-chatgpt\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai cohere tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "96d54b76-5844-465d-ae11-962d46019b86"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "# openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `system`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "# client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"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\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "fc7012a7-e315-486f-b906-10c13dadcf87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-8zygxEDkmvDoYNGSdRunoD1W3QMxO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex are two distinct programs serving different purposes:\\n\\n- LangChain: LangChain is a language learning platform that utilizes blockchain technology to facilitate language exchange between users. It connects language learners and speakers around the globe, allowing them to practice and improve their language skills through one-on-one conversations. LangChain aims to create a more immersive and effective language learning experience by providing real-time communication with native speakers.\\n\\n- LlamaIndex: LlamaIndex is a cryptocurrency index and analytics platform that offers insights and data on various digital assets and blockchain projects. It aggregates information from multiple sources to provide users with comprehensive market analysis, price tracking, and portfolio management tools. LlamaIndex is designed to help investors and traders make informed decisions in the volatile cryptocurrency market.', role='assistant', function_call=None, tool_calls=None))], created=1709781999, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_b9d4cef803', usage=CompletionUsage(completion_tokens=152, prompt_tokens=19, total_tokens=171))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "e871452b-0e60-4008-f700-7b1485a12641"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain is a blockchain project focused on using decentralized technology for language translation and interpretation services. It aims to create a platform that allows users to instantly access translation services from professional linguists.\n",
              "\n",
              "In contrast, LlamaIndex is a project that focuses on creating an index for the decentralized finance (DeFi) space. It aims to provide users with a comprehensive overview of various DeFi projects and tokens, allowing them to make informed decisions when investing in the DeFi market. \n",
              "\n",
              "Overall, the key difference between LangChain and LlamaIndex is their respective focuses on language translation services and DeFi project indexing."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `system` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The system message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the system prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "05b92e0a-38f0-4ff7-ef5c-b3c2a3383980"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I DON'T HAVE TIME FOR THIS NONSENSE! I JUST WANT FOOD NOW!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "d4ed7b30-36f4-4472-bd6d-c251bc5293e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I am delighted to answer your question today! I would say I prefer crushed ice because it's more fun to chew on and it cools drinks faster. How about you, do you have a preference for crushed or cubed ice?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "9106bf16-b795-4dbf-feeb-890033d82ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-8zymDwQhKrSYODimP01cCLUBKZZNN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I am delighted to answer your question today! I would say I prefer crushed ice because it's more fun to chew on and it cools drinks faster. How about you, do you have a preference for crushed or cubed ice?\", role='assistant', function_call=None, tool_calls=None))], created=1709782325, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_b9d4cef803', usage=CompletionUsage(completion_tokens=47, prompt_tokens=30, total_tokens=77))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "a2ec4d1d-c830-42dc-cb71-5479193212d3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "After a long day of work, all I wanted was a stimple dinner of falbean soup and fresh bread."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "91f341ea-0dd8-44c9-9d5e-df6b10ba8322"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I need a stimple falbean to fix this loose screw."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "b4e3d706-4e08-402b-c308-446b53f980dc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, it does matter which travel option Billy selects. If he chooses to fly and then take a bus, it will take him a total of 5 hours to get home. However, if he uses the teleporter and then takes a bus, it will only take him 1 hour to get home. Therefore, using the teleporter would be the faster option for Billy to get home before 7PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "HpKEt7Z5fo4s",
        "outputId": "c25a982f-afd5-4154-b2a2-388029f3e4e9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, it does matter which travel option Billy selects.\n",
              "\n",
              "First, let's consider the time it will take for Billy to get home via flying and taking a bus. \n",
              "Flying will take 3 hours, and then taking a bus will take 2 hours. \n",
              "So, the total time for flying and taking a bus is 3 hours (flight) + 2 hours (bus) = 5 hours. \n",
              "Since it's currently 1PM local time, if Billy chooses this option, he will arrive home at 6PM local time, which is before 7PM EDT.\n",
              "\n",
              "Next, let's consider the time it will take for Billy to get home via the teleporter and taking a bus. \n",
              "Using the teleporter will take 0 hours, and then taking a bus will take 1 hour. \n",
              "So, the total time for using the teleporter and taking a bus is 0 hours (teleporter) + 1 hour (bus) = 1 hour. \n",
              "If Billy chooses this option, he will arrive home at 2PM local time. \n",
              "This is earlier than the flying and bus option, but since Billy wants to get home before 7PM EDT, choosing the teleporter and bus option will still allow him to reach home in time.\n",
              "\n",
              "In conclusion, it does matter which travel option Billy selects, as both options will allow him to get home before 7PM EDT, but taking the teleporter and bus option will allow Billy to arrive home earlier at 2PM local time."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHH8zof-gkc1"
      },
      "source": [
        "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-3.5-turbo` through an API, developer style, let's move on to creating a simple application powered by `gpt-3.5-turbo`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/Beyond-ChatGPT/tree/main) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
